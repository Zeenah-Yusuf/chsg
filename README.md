# Community Health & Safety Guard (CHSG)

CHSG is a multimodal risk detection system that ingests **text, voice, and image reports**, runs them through trained ML models, applies risk logic, and triggers alerts (WhatsApp/TTS). A Streamlit dashboard provides live monitoring, and a scheduled job generates daily summaries.

## Project Structure

project/ 
â”œâ”€â”€ api/ # FastAPI backend 
â”‚ â””â”€â”€ app.py 
â”œâ”€â”€ dashboard/ # Streamlit dashboard 
â”‚ â””â”€â”€ app.py 
â”œâ”€â”€ jobs/ # Scheduled jobs 
â”‚ â”œâ”€â”€ daily_summary.py 
â”‚ â””â”€â”€ crontab 
â”œâ”€â”€ models/ # Trained ML models (.pkl) 
â”œâ”€â”€ schemas/ # JSON schemas for validation 
â”œâ”€â”€ data/ # Shared parquet files (latest_risk.parquet, daily_summary.parquet) 
â”œâ”€â”€ dockerfile/ 
â”‚ â”œâ”€â”€ Dockerfile # API 
â”‚ â”œâ”€â”€ Dashboard.Dockerfile # Dashboard 
â”‚ â””â”€â”€ Jobs.Dockerfile # Jobs 
â”œâ”€â”€ docker-compose.yml 
â””â”€â”€ requirements.txt

## Features

- **FastAPI backend**
  - `/predict/ndhs` â†’ Predict risk using NDHS model
  - `/predict/combined` â†’ Predict risk using combined model
  - `/ingest/text` â†’ Classify text reports
  - `/ingest/voice` â†’ Transcribe voice reports (Whisper)
  - `/ingest/image` â†’ Classify image reports (EfficientNet)
  - `/latest_risk` â†’ Serve latest risk records for dashboard

- **Streamlit dashboard**
  - Displays headline metrics (households at risk, risk rate)
  - Shows risk trend over time
  - Maps geographic distribution of risky households
  - Breaks down hazard categories

- **Jobs**
  - `daily_summary.py` runs daily to aggregate risk counts
  - Results saved into `data/daily_summary.parquet`
  - Cron job inside container executes script at midnight

## Setup

### 1. Clone repo
```bash
git clone https://github.com/your-org/chsg.git
cd chsg
2. Environment variables
Create .env file in project root:
USE_ALERT_STUB=true
ALERT_RECIPIENT=whatsapp:+2348012345678
TWILIO_ACCOUNT_SID=your_sid
TWILIO_AUTH_TOKEN=your_token
TWILIO_WHATSAPP_FROM=whatsapp:+14155238886
3. Build containers
docker-compose up â€“build
Docker Services
â€¢	API â†’ FastAPI backend at http://localhost:8000/docs
â€¢	Dashboard â†’ Streamlit dashboard at http://localhost:8501
â€¢	Jobs â†’ Cron container running daily_summary.py daily

requirements.txt (Pinned Versions)
fastapi==0.115.0
uvicorn[standard]==0.30.1
streamlit==1.39.0
pandas==2.2.2
numpy==1.26.4
joblib==1.4.2
scikit-learn==1.5.2
xgboost==2.1.1
transformers==4.44.2
torch==2.2.2
sentencepiece==0.2.0
edge-tts==6.1.9
soundfile==0.12.1
librosa==0.10.2.post1
Pillow==10.4.0
opencv-python==4.10.0.84
jsonschema==4.23.0
python-dotenv==1.0.1
twilio==9.2.3
httpx==0.27.0

Usage
API
curl -X POST http://localhost:8000/ingest/text \
  -H "Content-Type: application/json" \
  -d '{"text":"cholera outbreak near river", "lat":6.49, "lon":3.38}'
Dashboard
Visit http://localhost:8501 in your browser.
Jobs
Logs available in jobs container:
docker logs chsg_jobs

Data Flow
1.	Reports ingested via API endpoints.
2.	Predictions + risk flags generated by ML models.
3.	Records saved into data/latest_risk.parquet.
4.	Dashboard fetches /latest_risk or reads parquet directly.
5.	Jobs run daily summary â†’ data/daily_summary.parquet.

Extending
â€¢	Add new hazard mappings in image_classifier.py.
â€¢	Connect to a database (Postgres/SQLite) instead of parquet for persistence.
â€¢	Deploy with Kubernetes for scaling API and dashboard separately.

Maintainers
â€¢	Zeenah â€” Lead developer, Abuja ðŸ‡³ðŸ‡¬
â€¢	Contributions welcome via PRs.
